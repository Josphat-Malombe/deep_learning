{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0fvDXjEYbjl/I+bK/RVfB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josphat-Malombe/deep_learning/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Xat0ZOrCOa",
        "outputId": "39fd89e0-1150-4359-c2ae-4b050c7afb00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "directory exists\n",
            "downloading the file\n",
            "unzipping\n"
          ]
        }
      ],
      "source": [
        "#getting data\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "data_path=Path(\"/data\")\n",
        "image_path=data_path/\"pizza_steak_sushi\"\n",
        "\n",
        "if image_path.is_dir():\n",
        "  print(\"directory exists\")\n",
        "else:\n",
        "  print(\"creating the directory\")\n",
        "  image_path.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "with open(data_path/\"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request=requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "  print(\"downloading the file\")\n",
        "  f.write(request.content)\n",
        "\n",
        "with zipfile.ZipFile(data_path/\"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"unzipping\")\n",
        "  zip_ref.extractall(image_path)\n",
        "\n",
        "os.remove(data_path/ \"pizza_steak_sushi.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile main/data_setup.py\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "os.makedirs(\"main\",exist_ok=True)\n",
        "NUM_WORKERS=2\n",
        "\n",
        "\n",
        "def create_dataloader(\n",
        "  train_dir:str,\n",
        "  test_dir:str,\n",
        "  transform:transforms.Compose,\n",
        "   batch_size:int,\n",
        "   num_workers:int=NUM_WORKERS):\n",
        "\n",
        "\n",
        "  train_data=datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data=datasets.ImageFolder(test_dir,transform=None)\n",
        "\n",
        "\n",
        "  class_names=train_data.classes\n",
        "  train_dataloader=DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "  test_dataloader=DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "  return train_dataloader,test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnqOxq8_vbmF",
        "outputId": "2f48bf09-7122-463a-b132-073a8b128a8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main/model.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "class tinyVGG(nn.module):\n",
        "  def __init__(self, input_shape:int,hidden_units:int,output_shape:int)->None:\n",
        "    super().__init()\n",
        "    self.conv_block1=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool(kernel_size=2,stride=2)\n",
        "        )\n",
        "    self.conv_block1=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool(kernel_size=2,stride=2)\n",
        "        )\n",
        "    self.classifier=nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*13*13,out_features=output_shape)\n",
        "    )\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    x=self.conv_block1(x)\n",
        "    x=self.conv_block2(x)\n",
        "    x=self.classifier(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zOtDYbJR2DcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8de00b-6e81-437e-f427-447252672e8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main/engine.py\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List,Dict,Tuple\n",
        "def train_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,criterion:torch.nn.Module,optimizer:torch.optim.optimizer):\n",
        "  model.train()\n",
        "  train_loss,train_acc=0,0\n",
        "  for batch,(X,y) in enumerate(dataloader):\n",
        "    y_pred=model(X)\n",
        "    loss=criterion(y_pred,y)\n",
        "    train_loss+=loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    y_pred_class=torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n",
        "    train_acc+=(y_pred_class==y).sum().item()/len(y_pred)\n",
        "  train_loss=train_loss/len(dataloader)\n",
        "  train_acc=train_acc/len(dataloader)\n",
        "  return train_loss,train_acc\n",
        "def test_step(model:torch.nn.Module,dataloader:torch.utils.data.DataLoader,criterion:torch.nn.Module):\n",
        "  model.eval()\n",
        "  test_loss,test_acc=0,0\n",
        "  with torch.inference_mode():\n",
        "    for batch,(X,y) in enumerate(dataloader):\n",
        "      test_pred_logits=model(X)\n",
        "      loss=criterion(test_pred_logits,y)\n",
        "      test_loss+=loss.item()\n",
        "      test_pred_labels=test_pred_logits.argmax(dim=1)\n",
        "      test_acc+=((test_pred_labels==y).sum().item()/len(test_pred_labels))\n",
        "  test_loss=test_loss/len(dataloader)\n",
        "  test_acc=test_acc/len(dataloader)\n",
        "  return test_loss,test_acc\n",
        "\n",
        "def train(model:torch.nn.Module,\n",
        "          train_dataloader:torch.utils.data.DataLoader,\n",
        "          test_dataloader:torch.utils.data.DataLoader,\n",
        "          criterion:torch.nn.Module,\n",
        "          optimizer:torch.optim.Optimizers,\n",
        "          epochs:int,)->Dict(str,List):\n",
        "\n",
        "        results={\"train_loss\":[],\n",
        "                 \"train_acc\":[],\n",
        "                 \"test_loss\":[],\n",
        "                 \"test_acc\":[]\n",
        "                 }\n",
        "        for epoch in tqdm(range(epochs)):\n",
        "          train_loss,train_acc=train_step(model=model,dataloader=train_dataloader,criterion=criterion,optimizer=optimizer)\n",
        "          test_loss,test_acc=test_step(model=model,dataloader=test_dataloader,criterion=criterion)\n",
        "\n",
        "          print(\n",
        "              f\"epoch: {epoch+1} | \"\n",
        "              f\"train_loss: {train_loss} | \"\n",
        "              f\"train_acc: {train_acc} | \"\n",
        "              f\"test_loss: {test_loss} | \"\n",
        "              f\"test_acc: {test_acc} | \"\n",
        "          )\n",
        "\n",
        "          results[\"train_loss\"].append(train_loss)\n",
        "          results[\"train_acc\"].append(train_acc)\n",
        "          results[\"test_loss\"].append(test_loss)\n",
        "          results[\"test_acc\"].append(test_acc)\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga6mru4e1vjk",
        "outputId": "5404f41d-ef14-48f2-e84d-4dc84b191ee1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main/utils.py\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model:torch.nn.Module,\n",
        "               target_dir:str,\n",
        "               model_name:str):\n",
        "  target_dir_path=Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,exist_ok=True)\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model name should have extension .pt , .pth\"\n",
        "  model_save_path=target_dir_path/model_name\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu-lJEl192qR",
        "outputId": "5b47d2fe-9770-4861-9b29-3c54539b3439"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main/train.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import dataset_up,engine,model,utils\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "NUM_EPOCHS=5\n",
        "BATCH_SIZE=32\n",
        "HIDDEN_UNIT=10\n",
        "LEARNING_RATE=0.001\n",
        "\n",
        "train_dir=\"data/pizza_steak_sushi/train\"\n",
        "test_dir=\"data/pizza_steak_sushi/test\"\n",
        "\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "data_transform=transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader,test_dataloader,class_names=dataset_up.create_dataloader(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "model=model.tinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNIT,\n",
        "    output_shape=len(class_names)\n",
        ")\n",
        "criterion=torch.nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "engine.train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    epochs=NUM_EPOCHS,\n",
        ")\n",
        "utils.save_model(model=model,target_dir=\"models\",model_name=\"granular_model.pth\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMHz--EI_beV",
        "outputId": "e617c2f6-4607-46be-8191-aad668b8ffb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main/train.py\n"
          ]
        }
      ]
    }
  ]
}